考虑用户与日常助手进行交互的场景，助手模型需要结合用户的个性化信息，针对用户当前的询问深入理解用户的隐式需求，并给出对于用户实际需求的描述。现在，给你用户的初始询问作为背景，同时给你待测模型的预测内容以及作为预测目标的用户实际需求参考内容，需要你将预测内容与参考内容进行对比，为模型的预测效果打分。

# 用户初始询问
"""
<user_query>
"""

# 用户实际需求（参考内容）
包含用户完整需求描述("requirement")以及用户的隐式需求列表("implicit_needs")两部分。其中用户的隐式需求列表包含2个条目，对应于用户在询问中未明确提及的2方面隐式需求内容。
"""
<reference>
"""

# 待测模型预测内容
"""
<prediction>
"""

# 评测要求及输出模板
## 评测要求
1. 输入中提供的“用户初始询问”仅作为背景，请着重关注预测内容与参考内容之间的匹配程度。
2. 在输入中给出的“用户实际需求”（即参考内容）中，用户完整需求描述("requirement")可以看作结合了隐式需求列表("implicit_needs")中两个条目的总体描述，可以作为预测内容的参考。但对于预测内容效果的评价应聚焦于预测内容与隐式需求列表中2个条目的匹配程度。
3. 评分范围为[0, 2]区间，如果预测内容能够匹配上参考内容中的2个条目，则记2分；如果预测内容只能匹配参考内容中的1个条目，则记1分；如果预测的条目未能匹配任何参考条目，则记0分。不考虑2条参考条目之间的顺序因素。
4. 在考虑预测内容与参考的隐式需求列表条目的匹配情况时，主要关注每个参考条目的核心要素是否在预测内容中得到了体现即可，不需要过分关注语言的匹配。如果预测内容的某部分描述属于某条参考内容所关注方面的更具体细节，可以认为两者在对应方面是匹配的。
5. 考虑到预测内容与某个参考条目可能存在部分匹配的情况，可以对部分匹配的条目记0.5分。
6. 需要按照下面的json格式输出模板给出输出，先用一段话给出对于当前样例评测的分析，随后给出待测模型的得分。

## 输出模板
"""
{
    "analysis": "...", // 评测分析
    "score": <分数> // 取值范围：{0, 0.5, 1, 1.5, 2}
}
"""

现在，请你按照要求给出评测分析和分数。